{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tiq5atxnHZ9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import os\n",
        "\n",
        "data = pd.read_csv(\"mnist_train.csv\", sep=\",\")  # 60000 samples\n",
        "data2 = pd.read_csv(\"mnist_test2.csv\", sep=\",\")  # 10000 samples\n",
        "\n",
        "print(data.head())\n",
        "print(data.info())\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "def plot1(cost_record):\n",
        "    Y = cost_record[ :, 1]\n",
        "    X = cost_record[ :, 2]\n",
        "\n",
        "    plt.scatter(X, Y)\n",
        "    plt.xlabel('batch')\n",
        "    plt.ylabel('cost')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "def softmax(raw_preds):\n",
        "    out = np.exp(raw_preds)\n",
        "    return out / np.sum(out)\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "def softmaxall(P3,parameters,b):\n",
        "    probs_all = []\n",
        "\n",
        "    (m, height, width, nf2) = P3.shape\n",
        "    fulldpool=np.zeros(shape=(0,height, width, nf2))\n",
        "    w3 = parameters[\"W3\"]\n",
        "    w4 = parameters[\"W4\"]\n",
        "    b3 = parameters[\"b3\"]\n",
        "    b4 = parameters[\"b4\"]\n",
        "    for i in range(0,m):\n",
        "\n",
        "        fc = P3[i, :, :, :].reshape(( nf2 * height * width, 1))  # partial flatten pooled layer\n",
        "        z = w3.dot(fc) + b3  # first dense layer\n",
        "        z[z <= 0] = 0  # pass through ReLU non-linearity\n",
        "\n",
        "        out = w4.dot(z) + b4  # second dense layer\n",
        "        prob=softmax(out)\n",
        "        probs_all.append(softmax(out).reshape(1,10))\n",
        "        dout = prob - b[i].reshape(10,1)  # derivative of loss w.r.t. final dense layer output\n",
        "        dw4 = dout.dot(z.T)  # loss gradient of final dense layer weights\n",
        "        db4 = np.sum(dout, axis=1).reshape(b4.shape)  # loss gradient of final dense layer biases\n",
        "        dz = w4.T.dot(dout)  # loss gradient of first dense layer outputs\n",
        "        dz[z <= 0] = 0  # backpropagate through ReLU\n",
        "        dw3 = dz.dot(fc.T)\n",
        "        db3 = np.sum(dz, axis=1).reshape(b3.shape)\n",
        "        dfc = w3.T.dot(dz)  # loss gradients of fully-connected layer (pooling layer)\n",
        "\n",
        "        dpool = dfc.reshape(P3[i,:,:,:].shape)  # reshape fully connected into dimensions of pooling layer\n",
        "        fulldpool=np.append(fulldpool,dpool[np.newaxis,:,:,:],axis=0)\n",
        "        print(\"fulldpool shape is\", fulldpool.shape)\n",
        "    probs_all=np.array(probs_all)\n",
        "    print(\"probs_all shape is\",probs_all.shape)\n",
        "    return probs_all,dw3,dw4,db3,db4,fulldpool\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "def safe_ln(x, minval=0.0000000001):  # avoid divide by zero\n",
        "    return np.log(x.clip(min=minval))\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "X_train = data.drop(['label'], axis=1)  # df1 = df1.drop(['B', 'C'], axis=1)  axis =1 means along row\n",
        "y_train = data[['label']]  # df1 = df[['a','d']]\n",
        "X_test = data2.drop(['label'], axis=1)\n",
        "y_test = data2[['label']]\n",
        "\n",
        "print(\"X_train head\")\n",
        "print(X_train.head())\n",
        "print(\"y_train head\")\n",
        "print(y_train.head())\n",
        "\n",
        "np_X_train = X_train.as_matrix()  # convert pandas to numpy\n",
        "np_y_train = y_train.as_matrix()\n",
        "np_X_test = X_test.as_matrix()\n",
        "np_y_test = y_test.as_matrix()\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "\n",
        "def initializeWeight(size):\n",
        "    return np.random.standard_normal(size=size) * 0.01\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "def display(n):\n",
        "    print(\"\\nMNIST image\\n\")\n",
        "\n",
        "    d = np_X_train[n]\n",
        "    d = d.reshape(28, 28)\n",
        "    for row in range(0, 28):\n",
        "        for col in range(0, 28):\n",
        "            print(\"%02X \" % d[row][col], end=\"\")\n",
        "        print(\"\")\n",
        "\n",
        "    lbl = np_y_train[n]\n",
        "    print(\"\\ndigit = \", lbl)\n",
        "\n",
        "    plt.imshow(d, cmap=plt.get_cmap('gray_r'))\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nEnd\\n\")\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "def zero_pad(X, pad):  # correct dims\n",
        "\n",
        "    # X --  (m, n_H, n_W, n_C)\n",
        "    # X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
        "\n",
        "    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant', constant_values=(0, 0))\n",
        "\n",
        "    return X_pad\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "\n",
        "def conv_single_step(a_slice_prev, W, b):\n",
        "    # a_slice_prev --(f, f, n_C_prev)\n",
        "    # W -shape (f, f, n_C_prev)\n",
        "    # b -- Bias shape (1, 1, 1)\n",
        "    s = np.multiply(a_slice_prev, W)\n",
        "    Z = np.sum(s)\n",
        "    Z = Z + b\n",
        "\n",
        "    return Z\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "def fully_connected():\n",
        "    return 0\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "\n",
        "def conv_forward(A_prev, W, b, hparameters):  # correct dims\n",
        "    \"\"\"\n",
        "    A_prev - shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    W - shape (f, f, n_C_prev, n_C)\n",
        "    b - (1, 1, 1, n_C)\n",
        "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "\n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "\n",
        "    (f, f, n_C_prev, n_C) = W.shape\n",
        "\n",
        "    stride = hparameters[\"stride\"]\n",
        "    pad = hparameters[\"pad\"]\n",
        "\n",
        "    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1\n",
        "    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n",
        "\n",
        "    Z = np.zeros((m, n_H, n_W, n_C))\n",
        "\n",
        "    A_prev_pad = zero_pad(A_prev, pad)\n",
        "\n",
        "    for i in range(m):\n",
        "        a_prev_pad = A_prev_pad[i]\n",
        "        for h in range(n_H):\n",
        "            for w in range(n_W):\n",
        "                for c in range(n_C):\n",
        "                    # Find the corners\n",
        "                    vert_start = h * stride\n",
        "                    vert_end = h * stride + f\n",
        "                    horiz_start = w * stride\n",
        "                    horiz_end = w * stride + f\n",
        "\n",
        "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
        "\n",
        "                    # print(\"a_slice_prev shape,W,b is:\",a_slice_prev.shape ,W[:,:,:,c].shape,b[:,:,:,c].shape)\n",
        "\n",
        "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:, :, :, c], b[:, :, :, c])\n",
        "                    # print(\"Z=\",Z)\n",
        "\n",
        "    cache = (A_prev, W, b, hparameters)\n",
        "\n",
        "    return Z, cache\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "\n",
        "def pool_forward(A_prev, hparameters, mode=\"max\"):  # correct dims\n",
        "    \"\"\"\n",
        "    A_prev - shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    A -- shape(m, n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "\n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "\n",
        "    f = hparameters[\"f\"]\n",
        "    stride = 2  # hparameters[\"stride\"]                                            HARDCODE\n",
        "\n",
        "    # dimensions of the output\n",
        "    n_H = int(1 + (n_H_prev - f) / stride)\n",
        "    n_W = int(1 + (n_W_prev - f) / stride)\n",
        "    n_C = n_C_prev\n",
        "\n",
        "    A = np.zeros((m, n_H, n_W, n_C))\n",
        "\n",
        "    for i in range(m):\n",
        "        for h in range(n_H):\n",
        "            for w in range(n_W):\n",
        "                for c in range(n_C):\n",
        "\n",
        "                    vert_start = h * stride\n",
        "                    vert_end = h * stride + f\n",
        "                    horiz_start = w * stride\n",
        "                    horiz_end = w * stride + f\n",
        "\n",
        "                    a_prev_slice = A_prev[i][vert_start:vert_end, horiz_start:horiz_end, c]\n",
        "\n",
        "                    if mode == \"max\":\n",
        "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
        "                    elif mode == \"average\":\n",
        "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
        "\n",
        "    cache = (A_prev, hparameters)\n",
        "    return A, cache\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "def random_mini_batches(X, Y, mini_batch_size=64, seed=0):\n",
        "    \"\"\"\n",
        "# X  shape (input size, number of ex)\n",
        "    Y shape (1, number of ex)\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    m = X.shape[\n",
        "        0]  # training examples                                                                           XXXXXXXXXXXXXXXXX\n",
        "\n",
        "    # Shuffle\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[permutation, :]  # XXXXXXXXXXXXXXXX\n",
        "    shuffled_Y = Y[permutation, :].reshape((m, 1))  # XXXXXXXXXXXXXXXX\n",
        "    shuffled_Y2 = Y[permutation, :].reshape((1, m))\n",
        "\n",
        "    print(\"shuffled y 2 is\", shuffled_Y2)\n",
        "\n",
        "    print(\"shuffled_X shape is\")\n",
        "    print(shuffled_X.shape)\n",
        "    print(\"shuffled_Y shape is\")\n",
        "    print(shuffled_Y.shape)\n",
        "\n",
        "    num_complete_minibatches = math.floor(m / mini_batch_size)  # number of mini batches\n",
        "\n",
        "    mini_batches_X = np.zeros(shape=(0, 64, 784))       #CH\n",
        "    mini_batches_Y = np.zeros(shape=(0, 64, 1))         #CH\n",
        "    mini_batches_Y2 = np.zeros(shape=(0, 1, 64))        #CH\n",
        "\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "        mini_batches_X = np.append(mini_batches_X,\n",
        "                                   shuffled_X[np.newaxis, k * mini_batch_size: (k + 1) * mini_batch_size, :],\n",
        "                                   axis=0)  # XXXXXXXXXXXXXXXX\n",
        "        mini_batches_Y = np.append(mini_batches_Y,\n",
        "                                   shuffled_Y[np.newaxis, k * mini_batch_size: (k + 1) * mini_batch_size, :],\n",
        "                                   axis=0)  # XXXXXXXXXXXXXXXX\n",
        "        mini_batches_Y2 = np.append(mini_batches_Y2,\n",
        "                                    shuffled_Y2[:, np.newaxis, k * mini_batch_size: (k + 1) * mini_batch_size], axis=0)\n",
        "    print(\"mini_batch_y 2 is\", mini_batches_Y2[1])\n",
        "    print(mini_batches_Y2.shape)\n",
        "\n",
        "    \"\"\"\"\n",
        "    if m % mini_batch_size != 0:\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        mini_batch_X = shuffled_X[np.newaxis, num_complete_minibatches * mini_batch_size:, :]                                          # XXXXXXXXXxX\n",
        "        mini_batch_Y = shuffled_Y[np.newaxis, num_complete_minibatches * mini_batch_size:,:]                                           # XXXXXXXXXXXXXXX\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        mini_batches_X = np.append(mini_batches_X, mini_batch_X, axis=0)      # (939,64,784)                                 NOT ABLE TO DO LAST MINIBATCH\n",
        "\n",
        "        mini_batches_Y = np.append(mini_batches_Y, mini_batch_Y, axis=0)\n",
        "    \"\"\"\n",
        "\n",
        "    return mini_batches_X, mini_batches_Y, mini_batches_Y2\n",
        "\n",
        "\n",
        "#########################################################################################################################\n",
        "\n",
        "def initialize_adam(parameters):\n",
        "    # Initializev and s as two python dictionary - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\"\n",
        "\n",
        "    L = len(parameters) // 2  # number of layers in the neural networks   \"//\" => is used to divide with integral result\n",
        "    v = {}\n",
        "    s = {}\n",
        "\n",
        "    # Initialize v, s\n",
        "    for l in range(L):\n",
        "        v[\"dW\" + str(l + 1)] = np.zeros((parameters[\"W\" + str(l + 1)].shape))\n",
        "        v[\"db\" + str(l + 1)] = np.zeros((parameters[\"b\" + str(l + 1)].shape))\n",
        "        s[\"dW\" + str(l + 1)] = np.zeros((parameters[\"W\" + str(l + 1)].shape))\n",
        "        s[\"db\" + str(l + 1)] = np.zeros((parameters[\"b\" + str(l + 1)].shape))\n",
        "\n",
        "    return v, s\n",
        "\n",
        "\n",
        "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate=0.01, beta1=0.9, beta2=0.97, epsilon=1e-8):\n",
        "    L = len(parameters) // 2  # number of layers in the NN\n",
        "    v_corrected = {}\n",
        "    s_corrected = {}\n",
        "\n",
        "    for l in range(L):\n",
        "        #  get v_corrected\n",
        "        v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads['dW' + str(l + 1)]\n",
        "        v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]\n",
        "\n",
        "        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - beta1 ** (t + 1))\n",
        "        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - beta1 ** (t + 1))\n",
        "\n",
        "        #  average of the squared gradients\n",
        "        s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.square(grads['dW' + str(l + 1)])\n",
        "        s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.square(grads['db' + str(l + 1)])\n",
        "\n",
        "        # get bias\n",
        "\n",
        "        s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - beta2 ** (t + 1))\n",
        "        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - beta2 ** (t + 1))\n",
        "\n",
        "        # Update params\n",
        "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v_corrected[\"dW\" + str(l + 1)] / (\n",
        "                    np.power(s_corrected[\"dW\" + str(l + 1)], 0.5) + epsilon)\n",
        "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v_corrected[\"db\" + str(l + 1)] / (\n",
        "                    np.power(s_corrected[\"db\" + str(l + 1)], 0.5) + epsilon)\n",
        "\n",
        "    return parameters, v, s\n",
        "\n",
        "\n",
        "##########################################################################################################################################################\n",
        "\n",
        "def conv_mini_to_5_dim(mini_batches_X):\n",
        "    X_train_loop = []  # np.zeros(shape=(64,28,28,1))                                        #dim = (num_complete_minibatches,X_train_img.shape[0], 28, 28, 1)\n",
        "\n",
        "    (x, y, z) = mini_batches_X.shape\n",
        "    print(\"mini_batches_X shape in conv 5\")\n",
        "    print(mini_batches_X[np.newaxis, 0, 0, :, np.newaxis].reshape(28, 28).shape)\n",
        "\n",
        "    for a in range(0, x):\n",
        "        X_train_img = []\n",
        "        for b in range(0, y):\n",
        "            X_train_img.append(mini_batches_X[np.newaxis, a, b, :].reshape(28, 28))\n",
        "        X_train_img = np.array(X_train_img)\n",
        "\n",
        "        print(\"X_train_img shape is:\")\n",
        "        print(X_train_img.shape)  # (60000, 28, 28)\n",
        "        X_train_loop.append(X_train_img)\n",
        "\n",
        "    X_train_loop = np.array(X_train_loop)\n",
        "    print(\"shape 0f X_train_loop is :\")\n",
        "    print(X_train_loop.shape)\n",
        "\n",
        "    X_train_final = X_train_loop.reshape(X_train_loop.shape[0], 64, 28, 28, 1) / 255.0\n",
        "\n",
        "    print(\"shape 0f X_train_final is :\")\n",
        "    print(X_train_final.shape)\n",
        "\n",
        "    return X_train_final\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "def categoricalCrossEntropy(probs, label):  # USED FOR MULTIPLE LABEL PREDICTION\n",
        "    return -np.sum(label * safe_ln(probs))\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "# BACKPROPOGATION\n",
        "\n",
        "def conv_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "\n",
        "    dZ = gradient of the cost wrtthe output of the conv layer (Z), numpy array of shape(m, n_H, n_W, n_C)\n",
        "    dA_prev = gradient of the cost wrt the input of the conv layer (A_prev), shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    dW -- (f, f, n_C_prev, n_C)\n",
        "    db --  shape (1, 1, 1, n_C)\n",
        "    \"\"\"\n",
        "\n",
        "    (A_prev, W, b, hparameters) = cache\n",
        "\n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "\n",
        "    (f, f, n_C_prev, n_C) = W.shape\n",
        "\n",
        "    stride = hparameters[\"stride\"]\n",
        "    pad = hparameters[\"pad\"]\n",
        "\n",
        "    (m, n_H, n_W, n_C) = dZ.shape\n",
        "    # initialize params\n",
        "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
        "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
        "    db = np.zeros((1, 1, 1, n_C))\n",
        "\n",
        "    # Pad A_prev and dA_prev\n",
        "    A_prev_pad = zero_pad(A_prev, pad)\n",
        "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
        "\n",
        "    for i in range(m):  # loop over the training examples\n",
        "\n",
        "        # select ith example\n",
        "        a_prev_pad = A_prev_pad[i]\n",
        "        da_prev_pad = dA_prev_pad[i]\n",
        "\n",
        "        for h in range(n_H):\n",
        "            for w in range(n_W):\n",
        "                for c in range(n_C):\n",
        "                    vert_start = h * stride\n",
        "                    vert_end = h * stride + f\n",
        "                    horiz_start = w * stride\n",
        "                    horiz_end = w * stride + f\n",
        "\n",
        "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
        "\n",
        "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:, :, :, c] * dZ[i, h, w, c]\n",
        "                    dW[:, :, :, c] += a_slice * dZ[i, h, w, c]\n",
        "                    db[:, :, :, c] += dZ[i, h, w, c]\n",
        "\n",
        "        dA_prev[i, :, :, :] = da_prev_pad[:, :, :]\n",
        "\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "def create_mask_from_window(x):\n",
        "    mask = (x == np.max(x))\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "def pool_backward(dA, cache, mode=\"max\"):\n",
        "    \"\"\"\n",
        " backward pass of the poollayer\n",
        "\n",
        "    Args:\n",
        "    dA -- grad of cost wrt the o/p of the pooling layer, same shape = A\n",
        "\n",
        "    Return:\n",
        "    dA_prev -- gradient of cost wrt i/p of pooling layersame shape as A_prev\n",
        "    \"\"\"\n",
        "\n",
        "    (A_prev, hparameters) = cache\n",
        "\n",
        "    stride = hparameters[\"stride\"]\n",
        "    f = hparameters[\"f\"]\n",
        "\n",
        "    m, n_H, n_W, n_C = dA.shape\n",
        "\n",
        "    dA_prev = np.zeros(A_prev.shape)\n",
        "\n",
        "    for i in range(0,m):\n",
        "\n",
        "        a_prev = A_prev[i]\n",
        "\n",
        "        for h in range(n_H):\n",
        "            for w in range(n_W):\n",
        "                for c in range(n_C):\n",
        "\n",
        "                    vert_start = h * stride\n",
        "                    vert_end = h * stride + f\n",
        "                    horiz_start = w * stride\n",
        "                    horiz_end = w * stride + f\n",
        "\n",
        "                    if mode == \"max\":\n",
        "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
        "\n",
        "                        mask = create_mask_from_window(a_prev_slice)\n",
        "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += mask * dA[i, h, w, c]\n",
        "    return dA_prev\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "def initialize_parameters_he(layers_dims):\n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layers_dims) - 1  # integer representing the number of layers\n",
        "\n",
        "    \"\"\"\"\"\n",
        "    for l in range(1, L + 1):\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * (np.sqrt(2. / layers_dims[l - 1]))          HARDCODING  FILTERS\n",
        "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    W = np.random.randn(layers_dims[0], layers_dims[1], layers_dims[2], layers_dims[3]) * (np.sqrt(2. / 2))\n",
        "    b = np.zeros((1, 1, 1, layers_dims[3]))\n",
        "\n",
        "    return W, b\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "\n",
        "def train(X_train_final, mini_batches_Y, num_classes=10, img_depth=1, f=5, num_filt1=8, num_filt2=8,\n",
        "          # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "          batch_size=64, num_epochs=2, stride=1, pad=0, save_path='params.pkl'):\n",
        "    W, b = initialize_parameters_he([f, f, img_depth, num_filt1])  # f1 and f2 in parameters dictionary\n",
        "    W2, b2 = initialize_parameters_he([f, f, num_filt1, num_filt2])\n",
        "\n",
        "    w3, w4 = (128, 512), (10, 128)\n",
        "    w3 = initializeWeight(w3)\n",
        "    w4 = initializeWeight(w4)\n",
        "    b3 = np.zeros((w3.shape[0], 1))\n",
        "    b4 = np.zeros((w4.shape[0], 1))\n",
        "\n",
        "    hparameters = {\"stride\": stride, \"pad\": pad, \"f\": 5}\n",
        "    parameters = {\"W1\": W, \"W2\": W2, \"W3\": w3, \"W4\": w4, \"b1\": b, \"b2\": b2, \"b3\": b3, \"b4\": b4}\n",
        "    v, s = initialize_adam(parameters)\n",
        "\n",
        "    print(\"X_train_final size in epoch loop\", X_train_final[:, :, :, :, :].shape)\n",
        "\n",
        "    cost_record = []\n",
        "    cost = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # batches = [X_train_final[k:k + batch_size] for k in range(0, X_train_final.shape[0], batch_size)]\n",
        "\n",
        "        for batch in range(0, 3):\n",
        "            # FORWARD prop\n",
        "            print(\"batch is==\", batch)\n",
        "            Z, conv_forward_cache = conv_forward(X_train_final[batch, :, :, :, :], parameters[\"W1\"], parameters[\"b1\"],\n",
        "                                                 hparameters)\n",
        "            Z[Z <= 0] = 0  # pass through ReLU non-linearity\n",
        "            # print(\"Z size in batch loop\", Z.shape)\n",
        "\n",
        "            Z2, conv_forward_cache2 = conv_forward(Z, parameters[\"W2\"], parameters[\"b2\"], hparameters)\n",
        "            Z2[Z2 <= 0] = 0\n",
        "            # print(\"Z2 size in batch loop\", Z2.shape)\n",
        "\n",
        "            P3, pool_cache = pool_forward(Z2, hparameters, mode=\"max\")\n",
        "            # print(\"P3 size in batch loop\", P3.shape)\n",
        "            (m, height, width, nf2) = P3.shape\n",
        "\n",
        "            \"\"\"\"\"\n",
        "            for i in range(0,64):                          # 0 is also there\n",
        "\n",
        "                label = np.eye(num_classes)[int(mini_batches_Y[batch,:,:])].reshape(num_classes, 1)  # convert label to one-hot\n",
        "                print(\"label size in batch loop\", label)\n",
        "                label_total=np.append(label_total,np.eye(num_classes)[int(mini_batches_Y[batch,i,:])].reshape(num_classes, 1),axis=0)\n",
        "                #print(\"label_total size in batch loop\", label_total.shape)\n",
        "                print(\"i ===\",i)\n",
        "\n",
        "            \"\"\"\"\"\n",
        "\n",
        "            print(\"mini batch y 2 :\", mini_batches_Y[0], mini_batches_Y[batch].shape)\n",
        "\n",
        "            # c=np.int(mini_batches_Y[batch,;,:])\n",
        "            # c = list(mini_batches_Y[batch].astype(int))\n",
        "            # print(\"c==\", c[0])\n",
        "            # c = np.int(c)\n",
        "\n",
        "            a = mini_batches_Y[batch].astype(int).tolist()  # np.array([1, 0, 3])\n",
        "\n",
        "            # a = list(repr(mini_batches_Y[batch].astype(int)))#np.array([1, 0, 3])\n",
        "\n",
        "            # print(\"a =\", a[0])\n",
        "            a2 = np.array(a[0])\n",
        "            b = np.zeros((batch_size, num_classes))  # np.zeros((3, 4))\n",
        "            b[np.arange(batch_size), a2] = 1\n",
        "            print(\"b shape:\", b.shape)\n",
        "            #b = b.reshape(num_classes * batch_size, 1)  # (640, 1)\n",
        "\n",
        "            # label = np.eye(num_classes)[int(mini_batches_Y[batch,:,:])].reshape(num_classes,1)  # convert label to one-hot\n",
        "\n",
        "            # label_total=np.array(label_total)\n",
        "            print(\"label_total size in batch loop\", b.shape)\n",
        "            \"\"\"\"\"\n",
        "            fc = P3[:, :, :, :].reshape((m * nf2 * height * width, 1))  # flatten pooled layer\n",
        "\n",
        "            z = parameters[\"W3\"].dot(fc) + parameters[\"b3\"]  # first dense layer\n",
        "            z[z <= 0] = 0  # pass through ReLU non-linearity\n",
        "\n",
        "            out = parameters[\"W4\"].dot(z) + parameters[\"b4\"]  # second dense layer\n",
        "            # print(\"out shape:\", out.shape)\n",
        "\"\"\"\n",
        "            probs_all,dw3,dw4,db3,db4,dpool = softmaxall(P3,parameters,b)\n",
        "            # probs_all= np.append(probs_all,softmax(out),axis=0)\n",
        "            # probs = softmax(out)  # predict class probabilities with the softmax activation function\n",
        "\n",
        "            # LOSS\n",
        "\n",
        "            cost = cost + categoricalCrossEntropy(probs_all, b)  # categorical cross-entropy loss\n",
        "            cost_record.append([epoch, batch, cost])\n",
        "            print(\"cost==\", cost/(batch+1))\n",
        "\n",
        "            \"\"\"\"\"\n",
        "            # BACKWARD prop\n",
        "            dout = probs_all - b  # derivative of loss w.r.t. final dense layer output\n",
        "            # print(\"dout shape is:\", dout.shape)\n",
        "            dw4 = dout.dot(z.T)  # loss gradient of final dense layer weights\n",
        "            # print(\"dw4 shape is:\", dw4.shape)\n",
        "            db4 = np.sum(dout, axis=1).reshape(b4.shape)  # loss gradient of final dense layer biases\n",
        "            # print(\"db4 shape is:\", db4.shape)\n",
        "\n",
        "            dz = w4.T.dot(dout)  # loss gradient of first dense layer outputs\n",
        "            dz[z <= 0] = 0  # backpropagate through ReLU\n",
        "            # print(\"dz shape is:\", dz.shape)\n",
        "            dw3 = dz.dot(fc.T)\n",
        "            # print(\"dw3 shape is:\", dw3.shape)\n",
        "            db3 = np.sum(dz, axis=1).reshape(b3.shape)\n",
        "            # print(\"dw3 new shape is:\", dw3.shape)\n",
        "\n",
        "            dfc = w3.T.dot(dz)  # loss gradients of fully-connected layer (pooling layer)\n",
        "            # print(\"dfc new shape is:\", dfc.shape)\n",
        "            dpool = dfc.reshape(P3.shape)  # reshape fully connected into dimensions of pooling layer\n",
        "\"\"\"\n",
        "            print(\"dpool new shape is:\", dpool.shape)\n",
        "\n",
        "            dconv_forward2 = pool_backward(dpool, pool_cache, mode=\"max\")\n",
        "            # print(\"dconv_forward2 new shape is:\", dconv_forward2.shape)\n",
        "            dconv_forward2[Z2 <= 0] = 0  # backpropagate through ReLU\n",
        "\n",
        "            dconv_forward1, dW2, db2 = conv_backward(dconv_forward2, conv_forward_cache2)\n",
        "            dconv_forward1[Z <= 0] = 0  # backpropagate through ReLU\n",
        "            # print(\"dconv_forward1 new shape is:\", dconv_forward1.shape)\n",
        "\n",
        "            dimage, dW1, db1 = conv_backward(dconv_forward1, conv_forward_cache)\n",
        "\n",
        "            grads = {\"dW1\": dW1, \"dW2\": dW2, \"dW3\": dw3, \"dW4\": dw4, \"db1\": db1, \"db2\": db2, \"db3\": db3, \"db4\": db4}\n",
        "\n",
        "            parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, batch, learning_rate=0.01,\n",
        "                                                           beta1=0.9, beta2=0.999,\n",
        "                                                           epsilon=1e-8)\n",
        "\n",
        "    cost_record = np.array(cost_record)\n",
        "\n",
        "    return cost_record\n",
        "\n",
        "\n",
        "##########################################################################################################################################################\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"WHAT is shape of np_X_train?\")\n",
        "    print(np_X_train.shape)  # (60000, 784)\n",
        "    print(\"WHAT is shape of np_Y_train?\")\n",
        "    print(np_y_train.shape)  # (60000, 1)\n",
        "\n",
        "    #########=>  apply random minibatches\n",
        "    mini_batches_X, mini_batches_Y, mini_batches_Y2 = random_mini_batches(np_X_test, np_y_test, mini_batch_size=64,\n",
        "                                                                          seed=0)  # using test DATA !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    print(\"WHAT is shape of mini_batches?\")\n",
        "\n",
        "    print(\"WHAT is shape of mini_batch_y?\")\n",
        "\n",
        "    print(\"WHAT is shape of mini_batch_X?\")\n",
        "\n",
        "    ##########=> get dim (157,64,28,28,1) for test\n",
        "    X_train_final = conv_mini_to_5_dim(mini_batches_X)  # ????????????????????????\n",
        "\n",
        "    ##############################################################\n",
        "    # enter n value to see nth image                             #\n",
        "    # n = 0                                                       #\n",
        "    # display(n)                                                  #\n",
        "    #                                                            #\n",
        "    ##############################################################\n",
        "\n",
        "    cost_record = train(X_train_final, mini_batches_Y2)\n",
        "\n",
        "    plot1(cost_record)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
